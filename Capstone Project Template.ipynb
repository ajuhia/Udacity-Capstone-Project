{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "This project aims to create an ETL Pipeline for the US Immigration Dataset and other supplemantary datasets which includes: data on airport codes, U.S. city demographics, and temperature datasets for different cities. The ETL pipeline involves steps to fetch the raw datasets provided by the Udacity team, do exploratory data analysis on these datasets, process them and finally create analytics tables in a S3 datalake, which could further be utilised by the analytics team,<br>\n",
    ">##### To perform analysis to answer questions like:\n",
    "Which city has the highest records of being the port of entry by the immigrants in US? How long do immigrants tend to stay in US? What is the most common reason for the immigrants to travel to US?\n",
    ">##### To withdraw corelations like:\n",
    "What impact does temperature has on the count of number of immigrants? What is the relation between number of travels and city demographics in US?\n",
    "<br><br>\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 1.1 Scope \n",
    "<p>The purpose of the project is to build an end-to-end ETL pipeline to create a schema-on-read datalake. Raw data provide by the Udacity team is first fetched, cleaned, and processed to create analytics fact and dimension tables in a S3 datalake. As described above in some example use-cases, these tables can be further utilised to draw corelation within datasets and for analytics purpose.</p>\n",
    "\n",
    "Tools and technologies used in this project are: `Python libraries like Pandas, Numpy, Configparser, os, etc.`, `PySpark`, `AWS Services like AWS IAM, AWS S3`, `Jupyter Notebook` etc.<br>\n",
    "Pyspark and Pandas has been used to load data into dataframes, explore and process them. Then these processed datasets are stored in AWS S3 as fact and dimension tables in `parquet` file format using spark. Parquet files store data in columnar format, which have better performance on aggregation queries. Finally, data quality checks for Primary key constraints and rowcount is run on the above created tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import os\n",
    "import re\n",
    "import calendar\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import configparser\n",
    "import datetime as dt\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import udf, col, upper, year, month, dayofmonth, hour, weekofyear, date_format, isnan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Load Configuration Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth',100)\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "os.environ['AWS_S3_BUCKET']=config['AWS']['AWS_S3_BUCKET']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Define function to create Spark Session Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def createSparkSession():\n",
    "    \"\"\"\n",
    "         This function creates a spark session object and returns it.\n",
    "    \"\"\"\n",
    "    spark = SparkSession.builder\\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\")\\\n",
    "        .enableHiveSupport().getOrCreate()\n",
    "    \n",
    "    return spark   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 1.2 Describe and Gather Data \n",
    "Following datasets have been used in this project:\n",
    "- `I94 Immigration Data`:This data comes from the US National Tourism and Trade Office.Dataset consists of 12 files containing data for each month. Each file has around 3 million rows and 28 columns. A data dictionary explaining columns is also included at data/I94_SAS_Labels_Descriptions.SAS. [ NOTE: Data inside sas_data dir contains data for April 2016 and thus can also be used to perform this analysis. Similarly, in code we have processed data only for month of April. ]\n",
    "- `World Temperature Data`: This dataset came from Kaggle. This contains temperature data for different cities for countries across the world. For this project we have used the temperature recorded for only US cites in csv format. Also, since the provided dataset had data last data from year 2013, so we have extracted latest records only from this datset for various cities in US.\n",
    "- `U.S. City Demographic Data`: This data comes from OpenSoft and have been provided as csv file. It contains various demographics attribute for the cities in US like population count, their median age, average household size, etc.\n",
    "- `Airport Code Data`: This is a csv file of airport related data like airport codes, airport name, their location, etc.<br><br>\n",
    "[NOTE: Refer below for more details on these datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********IMMIGRATION DATA*************\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>XXX</td>\n",
       "      <td>20573.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>U</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1979.0</td>\n",
       "      <td>10282016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.897628e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.736796e+09</td>\n",
       "      <td>00296</td>\n",
       "      <td>F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>WAS</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MI</td>\n",
       "      <td>20691.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1961.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OS</td>\n",
       "      <td>6.666432e+08</td>\n",
       "      <td>93</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  i94mode i94addr  \\\n",
       "0    6.0  2016.0     4.0   692.0   692.0     XXX  20573.0      NaN     NaN   \n",
       "1    7.0  2016.0     4.0   254.0   276.0     ATL  20551.0      1.0      AL   \n",
       "2   15.0  2016.0     4.0   101.0   101.0     WAS  20545.0      1.0      MI   \n",
       "3   16.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "4   17.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "\n",
       "   depdate   ...     entdepu  matflag  biryear   dtaddto gender insnum  \\\n",
       "0      NaN   ...           U      NaN   1979.0  10282016    NaN    NaN   \n",
       "1      NaN   ...           Y      NaN   1991.0       D/S      M    NaN   \n",
       "2  20691.0   ...         NaN        M   1961.0  09302016      M    NaN   \n",
       "3  20567.0   ...         NaN        M   1988.0  09302016    NaN    NaN   \n",
       "4  20567.0   ...         NaN        M   2012.0  09302016    NaN    NaN   \n",
       "\n",
       "  airline        admnum  fltno visatype  \n",
       "0     NaN  1.897628e+09    NaN       B2  \n",
       "1     NaN  3.736796e+09  00296       F1  \n",
       "2      OS  6.666432e+08     93       B2  \n",
       "3      AA  9.246846e+10  00199       B2  \n",
       "4      AA  9.246846e+10  00199       B2  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(3096313, 28)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "*********TEMPERATURE DATA*************\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1743-11-01</td>\n",
       "      <td>6.068</td>\n",
       "      <td>1.737</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1743-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1744-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1744-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1744-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt  AverageTemperature  AverageTemperatureUncertainty   City  \\\n",
       "0  1743-11-01               6.068                          1.737  Århus   \n",
       "1  1743-12-01                 NaN                            NaN  Århus   \n",
       "2  1744-01-01                 NaN                            NaN  Århus   \n",
       "3  1744-02-01                 NaN                            NaN  Århus   \n",
       "4  1744-03-01                 NaN                            NaN  Århus   \n",
       "\n",
       "   Country Latitude Longitude  \n",
       "0  Denmark   57.05N    10.33E  \n",
       "1  Denmark   57.05N    10.33E  \n",
       "2  Denmark   57.05N    10.33E  \n",
       "3  Denmark   57.05N    10.33E  \n",
       "4  Denmark   57.05N    10.33E  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(8599212, 7)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "*********US CITIES DEMOGRAPHICS DATA*************\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601.0</td>\n",
       "      <td>41862.0</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>30908.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129.0</td>\n",
       "      <td>49500.0</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147.0</td>\n",
       "      <td>32935.0</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040.0</td>\n",
       "      <td>46799.0</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819.0</td>\n",
       "      <td>8229.0</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>4759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127.0</td>\n",
       "      <td>87105.0</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821.0</td>\n",
       "      <td>33878.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040.0</td>\n",
       "      <td>143873.0</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829.0</td>\n",
       "      <td>86253.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City          State  Median Age  Male Population  \\\n",
       "0     Silver Spring       Maryland        33.8          40601.0   \n",
       "1            Quincy  Massachusetts        41.0          44129.0   \n",
       "2            Hoover        Alabama        38.5          38040.0   \n",
       "3  Rancho Cucamonga     California        34.5          88127.0   \n",
       "4            Newark     New Jersey        34.6         138040.0   \n",
       "\n",
       "   Female Population  Total Population  Number of Veterans  Foreign-born  \\\n",
       "0            41862.0             82463              1562.0       30908.0   \n",
       "1            49500.0             93629              4147.0       32935.0   \n",
       "2            46799.0             84839              4819.0        8229.0   \n",
       "3            87105.0            175232              5821.0       33878.0   \n",
       "4           143873.0            281913              5829.0       86253.0   \n",
       "\n",
       "   Average Household Size State Code                       Race  Count  \n",
       "0                    2.60         MD         Hispanic or Latino  25924  \n",
       "1                    2.39         MA                      White  58723  \n",
       "2                    2.58         AL                      Asian   4759  \n",
       "3                    3.18         CA  Black or African-American  24437  \n",
       "4                    2.73         NJ                      White  76402  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(2891, 12)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "*********AIRPORT CODES DATA*************\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00A</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Total Rf Heliport</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-PA</td>\n",
       "      <td>Bensalem</td>\n",
       "      <td>00A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00A</td>\n",
       "      <td>-74.93360137939453, 40.07080078125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00AA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Aero B Ranch Airport</td>\n",
       "      <td>3435.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-KS</td>\n",
       "      <td>Leoti</td>\n",
       "      <td>00AA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AA</td>\n",
       "      <td>-101.473911, 38.704022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00AK</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Lowell Field</td>\n",
       "      <td>450.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Anchor Point</td>\n",
       "      <td>00AK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AK</td>\n",
       "      <td>-151.695999146, 59.94919968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00AL</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Epps Airpark</td>\n",
       "      <td>820.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AL</td>\n",
       "      <td>Harvest</td>\n",
       "      <td>00AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AL</td>\n",
       "      <td>-86.77030181884766, 34.86479949951172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00AR</td>\n",
       "      <td>closed</td>\n",
       "      <td>Newport Hospital &amp; Clinic Heliport</td>\n",
       "      <td>237.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AR</td>\n",
       "      <td>Newport</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-91.254898, 35.6087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ident           type                                name  elevation_ft  \\\n",
       "0   00A       heliport                   Total Rf Heliport          11.0   \n",
       "1  00AA  small_airport                Aero B Ranch Airport        3435.0   \n",
       "2  00AK  small_airport                        Lowell Field         450.0   \n",
       "3  00AL  small_airport                        Epps Airpark         820.0   \n",
       "4  00AR         closed  Newport Hospital & Clinic Heliport         237.0   \n",
       "\n",
       "  continent iso_country iso_region  municipality gps_code iata_code  \\\n",
       "0       NaN          US      US-PA      Bensalem      00A       NaN   \n",
       "1       NaN          US      US-KS         Leoti     00AA       NaN   \n",
       "2       NaN          US      US-AK  Anchor Point     00AK       NaN   \n",
       "3       NaN          US      US-AL       Harvest     00AL       NaN   \n",
       "4       NaN          US      US-AR       Newport      NaN       NaN   \n",
       "\n",
       "  local_code                            coordinates  \n",
       "0        00A     -74.93360137939453, 40.07080078125  \n",
       "1       00AA                 -101.473911, 38.704022  \n",
       "2       00AK            -151.695999146, 59.94919968  \n",
       "3       00AL  -86.77030181884766, 34.86479949951172  \n",
       "4        NaN                    -91.254898, 35.6087  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(55075, 12)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Read Immigration Data\n",
    "print(\"*********IMMIGRATION DATA*************\")\n",
    "sas_df = pd.read_sas('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat', 'sas7bdat', encoding=\"ISO-8859-1\")\n",
    "display(sas_df.head(5),sas_df.shape)\n",
    "\n",
    "#Read Temperature Data\n",
    "print(\"\\n\\n*********TEMPERATURE DATA*************\")\n",
    "temp_df = pd.read_csv('../../data2/GlobalLandTemperaturesByCity.csv')\n",
    "display(temp_df.head(5),temp_df.shape)\n",
    "\n",
    "#Read US Cities Demographics Data\n",
    "print(\"\\n\\n*********US CITIES DEMOGRAPHICS DATA*************\")\n",
    "city_df = pd.read_csv('us-cities-demographics.csv',sep = ';')\n",
    "display(city_df.head(5),city_df.shape)\n",
    "\n",
    "#Read Airport Codes Data\n",
    "print(\"\\n\\n*********AIRPORT CODES DATA*************\")\n",
    "airport_df = pd.read_csv('airport-codes_csv.csv')\n",
    "display(airport_df.head(5),airport_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 2.1 Data Exploration\n",
    "Data quality issues: All datasets have checked below for datatypes, missing values, duplicate records, invalid values, etc.\n",
    "#### 2.2 Data Cleaning\n",
    "Data transformation and handling of above mentioned issue has been done while creating fact and dimension tables in the ETL Pipeline creation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### US immigration data\n",
    "- Date columns are present in SAS format. They need to be transformed.\n",
    "- Invalid destination cities: missing port code or ports of entry don't belong to US\n",
    "- Missing values for many columns that we will be using as per our data model: i94addr,departure date,fltno. (We will drop other cols as they are not going to be used)\n",
    "- No Duplicated rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cicid       float64\n",
       "i94yr       float64\n",
       "i94mon      float64\n",
       "i94cit      float64\n",
       "i94res      float64\n",
       "i94port      object\n",
       "arrdate     float64\n",
       "i94mode     float64\n",
       "i94addr      object\n",
       "depdate     float64\n",
       "i94bir      float64\n",
       "i94visa     float64\n",
       "count       float64\n",
       "dtadfile     object\n",
       "visapost     object\n",
       "occup        object\n",
       "entdepa      object\n",
       "entdepd      object\n",
       "entdepu      object\n",
       "matflag      object\n",
       "biryear     float64\n",
       "dtaddto      object\n",
       "gender       object\n",
       "insnum       object\n",
       "airline      object\n",
       "admnum      float64\n",
       "fltno        object\n",
       "visatype     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check column types\n",
    "sas_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cicid             0\n",
       "i94yr             0\n",
       "i94mon            0\n",
       "i94cit            0\n",
       "i94res            0\n",
       "i94port           0\n",
       "arrdate           0\n",
       "i94mode         239\n",
       "i94addr      152372\n",
       "depdate      142457\n",
       "i94bir          802\n",
       "i94visa           0\n",
       "count             0\n",
       "dtadfile          1\n",
       "visapost    1881250\n",
       "occup       3088187\n",
       "entdepa         238\n",
       "entdepd      138429\n",
       "entdepu     3095921\n",
       "matflag      138429\n",
       "biryear         802\n",
       "dtaddto         477\n",
       "gender       414269\n",
       "insnum      2982605\n",
       "airline       83627\n",
       "admnum            0\n",
       "fltno         19549\n",
       "visatype          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check for null Values\n",
    "sas_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate row count:Empty DataFrame\n",
      "Columns: [cicid, i94yr, i94mon, i94cit, i94res, i94port, arrdate, i94mode, i94addr, depdate, i94bir, i94visa, count, dtadfile, visapost, occup, entdepa, entdepd, entdepu, matflag, biryear, dtaddto, gender, insnum, airline, admnum, fltno, visatype]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "#check for duplicates\n",
    "duplicateRows = sas_df[sas_df.duplicated()]\n",
    "print('Duplicate row count:'+str(duplicateRows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Cities temperature data\n",
    "- dt column needs to be renamed and changed to datetime format\n",
    "- this dataset has temperature records for citeies all over the world, but as per our data model\n",
    "   we are interested in US cities only\n",
    "- rename columns as per datamodel and change to lowercase for merging with demographics dataset\n",
    "- last data was updated in year 2013, but immigration data was provided for 2016. Thus, we will extract latest records only.\n",
    "- AverageTemprature and AverageTemperatureUncertainty has missing values which needs to be handled.\n",
    "- No Duplicated rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dt                                object\n",
       "AverageTemperature               float64\n",
       "AverageTemperatureUncertainty    float64\n",
       "City                              object\n",
       "Country                           object\n",
       "Latitude                          object\n",
       "Longitude                         object\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check column types\n",
    "temp_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dt                                    0\n",
       "AverageTemperature               364130\n",
       "AverageTemperatureUncertainty    364130\n",
       "City                                  0\n",
       "Country                               0\n",
       "Latitude                              0\n",
       "Longitude                             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check for null Values\n",
    "temp_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate row count:Empty DataFrame\n",
      "Columns: [dt, AverageTemperature, AverageTemperatureUncertainty, City, Country, Latitude, Longitude]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#check for duplicates\n",
    "duplicateRows = temp_df[temp_df.duplicated()]\n",
    "print('Duplicate row count:'+str(duplicateRows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1743, 1744, 1745, 1746, 1747, 1748, 1749, 1750, 1751, 1752, 1753,\n",
       "       1754, 1755, 1756, 1757, 1758, 1759, 1760, 1761, 1762, 1763, 1764,\n",
       "       1765, 1766, 1767, 1768, 1769, 1770, 1771, 1772, 1773, 1774, 1775,\n",
       "       1776, 1777, 1778, 1779, 1780, 1781, 1782, 1783, 1784, 1785, 1786,\n",
       "       1787, 1788, 1789, 1790, 1791, 1792, 1793, 1794, 1795, 1796, 1797,\n",
       "       1798, 1799, 1800, 1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808,\n",
       "       1809, 1810, 1811, 1812, 1813, 1814, 1815, 1816, 1817, 1818, 1819,\n",
       "       1820, 1821, 1822, 1823, 1824, 1825, 1826, 1827, 1828, 1829, 1830,\n",
       "       1831, 1832, 1833, 1834, 1835, 1836, 1837, 1838, 1839, 1840, 1841,\n",
       "       1842, 1843, 1844, 1845, 1846, 1847, 1848, 1849, 1850, 1851, 1852,\n",
       "       1853, 1854, 1855, 1856, 1857, 1858, 1859, 1860, 1861, 1862, 1863,\n",
       "       1864, 1865, 1866, 1867, 1868, 1869, 1870, 1871, 1872, 1873, 1874,\n",
       "       1875, 1876, 1877, 1878, 1879, 1880, 1881, 1882, 1883, 1884, 1885,\n",
       "       1886, 1887, 1888, 1889, 1890, 1891, 1892, 1893, 1894, 1895, 1896,\n",
       "       1897, 1898, 1899, 1900, 1901, 1902, 1903, 1904, 1905, 1906, 1907,\n",
       "       1908, 1909, 1910, 1911, 1912, 1913, 1914, 1915, 1916, 1917, 1918,\n",
       "       1919, 1920, 1921, 1922, 1923, 1924, 1925, 1926, 1927, 1928, 1929,\n",
       "       1930, 1931, 1932, 1933, 1934, 1935, 1936, 1937, 1938, 1939, 1940,\n",
       "       1941, 1942, 1943, 1944, 1945, 1946, 1947, 1948, 1949, 1950, 1951,\n",
       "       1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962,\n",
       "       1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973,\n",
       "       1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984,\n",
       "       1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995,\n",
       "       1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006,\n",
       "       2007, 2008, 2009, 2010, 2011, 2012, 2013])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check for the values for year\n",
    "print(type(temp_df.dt[0]))\n",
    "temp_df.rename(columns ={'dt':'date'}, inplace=True)\n",
    "temp_df['date'] = pd.to_datetime(temp_df['date'])\n",
    "temp_df['date'].dt.year.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Abilene', 'Akron', 'Albuquerque', 'Alexandria', 'Allentown',\n",
       "       'Amarillo', 'Anaheim', 'Anchorage', 'Ann Arbor', 'Antioch',\n",
       "       'Arlington', 'Arvada', 'Atlanta', 'Aurora', 'Austin', 'Bakersfield',\n",
       "       'Baltimore', 'Baton Rouge', 'Beaumont', 'Bellevue', 'Berkeley',\n",
       "       'Birmingham', 'Boston', 'Bridgeport', 'Brownsville', 'Buffalo',\n",
       "       'Burbank', 'Cambridge', 'Cape Coral', 'Carrollton', 'Cary',\n",
       "       'Cedar Rapids', 'Chandler', 'Charleston', 'Charlotte',\n",
       "       'Chattanooga', 'Chesapeake', 'Chicago', 'Chula Vista', 'Cincinnati',\n",
       "       'Clarksville', 'Clearwater', 'Cleveland', 'Colorado Springs',\n",
       "       'Columbia', 'Columbus', 'Concord', 'Coral Springs', 'Corona',\n",
       "       'Corpus Christi', 'Costa Mesa', 'Dallas', 'Dayton', 'Denton',\n",
       "       'Denver', 'Des Moines', 'Detroit', 'Downey', 'Durham',\n",
       "       'East Los Angeles', 'Edison', 'El Monte', 'El Paso', 'Elizabeth',\n",
       "       'Escondido', 'Eugene', 'Evansville', 'Fairfield', 'Fayetteville',\n",
       "       'Flint', 'Fontana', 'Fort Collins', 'Fort Lauderdale', 'Fort Wayne',\n",
       "       'Fort Worth', 'Fremont', 'Fresno', 'Fullerton', 'Gainesville',\n",
       "       'Garden Grove', 'Garland', 'Gilbert', 'Glendale', 'Grand Prairie',\n",
       "       'Grand Rapids', 'Green Bay', 'Greensboro', 'Hampton', 'Hartford',\n",
       "       'Hayward', 'Henderson', 'Hialeah', 'Highlands Ranch', 'Hollywood',\n",
       "       'Houston', 'Huntington Beach', 'Huntsville', 'Independence',\n",
       "       'Indianapolis', 'Inglewood', 'Irvine', 'Irving', 'Jackson',\n",
       "       'Jacksonville', 'Jersey City', 'Joliet', 'Kansas City', 'Killeen',\n",
       "       'Knoxville', 'Lafayette', 'Lakewood', 'Lancaster', 'Lansing',\n",
       "       'Laredo', 'Las Vegas', 'Lexington Fayette', 'Lincoln',\n",
       "       'Little Rock', 'Long Beach', 'Los Angeles', 'Louisville', 'Lowell',\n",
       "       'Lubbock', 'Madison', 'Manchester', 'Memphis', 'Mesa', 'Mesquite',\n",
       "       'Metairie', 'Miami', 'Milwaukee', 'Minneapolis', 'Miramar',\n",
       "       'Mobile', 'Modesto', 'Montgomery', 'Moreno Valley', 'Naperville',\n",
       "       'Nashville', 'New Haven', 'New Orleans', 'New York', 'Newark',\n",
       "       'Newport News', 'Nogales', 'Norfolk', 'Norman', 'North Las Vegas',\n",
       "       'Norwalk', 'Nuevo Laredo', 'Oakland', 'Oceanside', 'Oklahoma City',\n",
       "       'Olathe', 'Omaha', 'Ontario', 'Orange', 'Orlando', 'Overland Park',\n",
       "       'Oxnard', 'Palmdale', 'Paradise', 'Pasadena', 'Paterson',\n",
       "       'Pembroke Pines', 'Peoria', 'Philadelphia', 'Phoenix', 'Pittsburgh',\n",
       "       'Plano', 'Pomona', 'Port Saint Lucie', 'Portland', 'Providence',\n",
       "       'Provo', 'Pueblo', 'Raleigh', 'Rancho Cucamonga', 'Reno', 'Rialto',\n",
       "       'Richardson', 'Richmond', 'Riverside', 'Rochester', 'Rockford',\n",
       "       'Roseville', 'Sacramento', 'Saint Louis', 'Saint Paul',\n",
       "       'Saint Petersburg', 'Salem', 'Salinas', 'Salt Lake City',\n",
       "       'San Antonio', 'San Bernardino', 'San Diego', 'San Francisco',\n",
       "       'San Jose', 'Santa Ana', 'Santa Clara', 'Santa Clarita',\n",
       "       'Santa Rosa', 'Savannah', 'Scottsdale', 'Seattle', 'Shreveport',\n",
       "       'Simi Valley', 'Sioux Falls', 'South Bend', 'Spokane',\n",
       "       'Spring Valley', 'Springfield', 'Stamford', 'Sterling Heights',\n",
       "       'Stockton', 'Sunnyvale', 'Sunrise Manor', 'Syracuse', 'Tacoma',\n",
       "       'Tallahassee', 'Tampa', 'Tempe', 'Thornton', 'Thousand Oaks',\n",
       "       'Toledo', 'Toms River', 'Topeka', 'Torrance', 'Tucson', 'Tulsa',\n",
       "       'Vallejo', 'Vancouver', 'Virginia Beach', 'Visalia', 'Waco',\n",
       "       'Warren', 'Washington', 'Waterbury', 'West Covina', 'West Jordan',\n",
       "       'West Valley City', 'Westminster', 'Wichita', 'Wichita Falls',\n",
       "       'Windsor', 'Winston Salem', 'Worcester', 'Yonkers'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check for unique city values\n",
    "temp = temp_df[temp_df.Country ==\"United States\"]\n",
    "temp.City.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### US Cities Demographics data\n",
    "- columns names have whitespce, '-' and to be transformed/renamed to lowercase or as per data model schema.\n",
    "- very few null values present for a few columns like Male Population, Female Poulation, no. of veterans etc,\n",
    "  which can be dropped\n",
    "- Duplicate rows are present due to Race and Count columns which can be filtered out as we don't need them as per data model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "City                       object\n",
       "State                      object\n",
       "Median Age                float64\n",
       "Male Population           float64\n",
       "Female Population         float64\n",
       "Total Population            int64\n",
       "Number of Veterans        float64\n",
       "Foreign-born              float64\n",
       "Average Household Size    float64\n",
       "State Code                 object\n",
       "Race                       object\n",
       "Count                       int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check column types\n",
    "city_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "City                       0\n",
       "State                      0\n",
       "Median Age                 0\n",
       "Male Population            3\n",
       "Female Population          3\n",
       "Total Population           0\n",
       "Number of Veterans        13\n",
       "Foreign-born              13\n",
       "Average Household Size    16\n",
       "State Code                 0\n",
       "Race                       0\n",
       "Count                      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check for null Values\n",
    "city_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate row count for city:City                      2324\n",
      "State                     2324\n",
      "Median Age                2324\n",
      "Male Population           2322\n",
      "Female Population         2322\n",
      "Total Population          2324\n",
      "Number of Veterans        2318\n",
      "Foreign-born              2318\n",
      "Average Household Size    2316\n",
      "State Code                2324\n",
      "Race                      2324\n",
      "Count                     2324\n",
      "dtype: int64\n",
      "Duplicate row count for city,state:City                      2295\n",
      "State                     2295\n",
      "Median Age                2295\n",
      "Male Population           2293\n",
      "Female Population         2293\n",
      "Total Population          2295\n",
      "Number of Veterans        2289\n",
      "Foreign-born              2289\n",
      "Average Household Size    2287\n",
      "State Code                2295\n",
      "Race                      2295\n",
      "Count                     2295\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#check for duplicates\n",
    "duplicateRows = city_df[city_df.duplicated('City')]\n",
    "print('Duplicate row count for city:'+str(duplicateRows.count()))\n",
    "\n",
    "duplicateRows = city_df[city_df.duplicated(['City','State'])]\n",
    "print('Duplicate row count for city,state:'+str(duplicateRows.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Airport Codes data\n",
    "-  columns are to be renamed and datatypes are to be changed as per the data model\n",
    "- No Duplicate records\n",
    "- Drop column iata_code as more than 80% records are null for this column.\n",
    "- Handle missing values for contient column by populating null values based on other rows for same column using column\n",
    "  iso_country.\n",
    "- Create two new columns from coordinate column i.e 'latitude' and 'longitude' and round data upto two decimal values.\n",
    "  Hence coordinate column is also to be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ident            object\n",
       "type             object\n",
       "name             object\n",
       "elevation_ft    float64\n",
       "continent        object\n",
       "iso_country      object\n",
       "iso_region       object\n",
       "municipality     object\n",
       "gps_code         object\n",
       "iata_code        object\n",
       "local_code       object\n",
       "coordinates      object\n",
       "dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check column types\n",
    "airport_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ident               0\n",
       "type                0\n",
       "name                0\n",
       "elevation_ft     7006\n",
       "continent       27719\n",
       "iso_country       247\n",
       "iso_region          0\n",
       "municipality     5676\n",
       "gps_code        14045\n",
       "iata_code       45886\n",
       "local_code      26389\n",
       "coordinates         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check for null Values\n",
    "airport_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate row count:Empty DataFrame\n",
      "Columns: [ident, type, name, elevation_ft, continent, iso_country, iso_region, municipality, gps_code, iata_code, local_code, coordinates]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#check for duplicates\n",
    "duplicateRows = airport_df[airport_df.duplicated()]\n",
    "print('Duplicate row count:'+str(duplicateRows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan 'OC' 'AF' 'AN' 'EU' 'AS' 'SA']\n",
      "['US-PA' 'US-KS' 'US-AK' 'US-AL' 'US-AR' 'US-OK' 'US-AZ' 'US-CA' 'US-CO'\n",
      " 'US-FL' 'US-GA' 'US-HI' 'US-ID' 'US-IN' 'US-IL' 'US-KY' 'US-LA' 'US-MD'\n",
      " 'US-MI' 'US-MN' 'US-MO' 'US-MT' 'US-NJ' 'US-NC' 'US-NY' 'US-OH' 'US-OR'\n",
      " 'US-SC' 'US-SD' 'US-TX' 'US-TN' 'US-UT' 'US-VA' 'US-WA' 'US-WI' 'US-WV'\n",
      " 'US-WY' 'US-CT' 'US-IA' 'US-MA' 'US-ME' 'US-NE' 'US-NH' 'US-NM' 'US-NV'\n",
      " 'US-MS' 'US-ND' 'US-VT' 'US-RI' 'US-DC']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([nan, 'AS'], dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check for unique valid values to populate for missing values\n",
    "print(airport_df.continent.unique())\n",
    "air_df = airport_df[airport_df.iso_country== 'US']\n",
    "arr=air_df.iso_region.unique()\n",
    "print(arr[:50])\n",
    "air_df.continent.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Below is the conceptual design for the analytical tables built in AWS S3 datalake. The following ***design strategies*** have been incorporated:\n",
    " 1. A **fact constellation schema** has been designed, to allow heavy analytical query processing on the datalake.\n",
    " 2. Multiple facts tables and shared dimension tables haven been created in S3 datalake to model the datasets in this schema.\n",
    " 3. The different components of fact constellation schema incude:\n",
    " \n",
    "    - **Fact Tables**: fact_us_cities_demographics, fact_immigration, fact_airport<br>\n",
    "    - **Dimension Table**: dim_calendar, dim_visa, dim_country, dim_state, dim_travelmode, dim_flight\n",
    "    \n",
    " 4. These tables have been partitioned on different columns to achive storage and data retrival optimisation.\n",
    " \n",
    "<img src=images/ERD.png>\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines: List the steps necessary to pipeline the data into the chosen data model\n",
    "- for ***fact_immigration*** columns are renamed as per the data model, date columns are transformed from sas date format to datetime date format and data is saved by creating partions on arrival_date.\n",
    "- ***dim_flight*** is cleaned by dropping duplicates and filtering null values for the primary key.\n",
    "- ***dim_calendar*** is cleaned by dropping duplicates and data is partitioned by year and month\n",
    "- fact_us_cities_demographics is created using us cities demographics dataset and by fetching latest temp records from temperature dataset for corresponding cities. Data is cleaned by filtering out columns that are not required and handling missing values and transforming data as per defined schema.\n",
    "- ***fact_airport*** is created from airport codes data, by populating null values based on other rows for same column, renaming columns as per defined schema, dropping unecessary columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model.\n",
    "1. Fetch the config details like AWS Credentials, AWS S3 Bucket path from the config file.\n",
    "2. Load the provided raw datasets using python pandas and pyspark session object, perform data cleaning by filtering null values and data transformation on these datasets to create following tables as per above defined data model: fact_us_cities_demographics, fact_immigration, fact_airport, dim_calendar, dim_flight.\n",
    "3. Parse file I94_SAS_Labels_Descriptions.SAS file and process the data by dropping duplicates and by removing missing values to create following dimension tables dim_visa, dim_country, dim_state, dim_travelmode\n",
    "4. Save all the above created tables in AWS S3 in paraquet format.\n",
    "5. Perform data quality checks on these tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "###### Immigration SAS Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def processUSImmigrationData(sas_input_path_prefix,sas_input_path_suffix,spark,output_path):\n",
    "    '''\n",
    "        This function reads SAS data,processes it and saves it parquet format to create fact_immigration and dimension tables:dim_flight and dim_calendar.\n",
    "        Input: prefix and suffix strings to calculate input path, spark session object, S3 path.\n",
    "        Output: Print out result.\n",
    "    '''\n",
    "    #Create a list of columns to extracted from SAS dataset and dict to rename these columns as per our data model\n",
    "    sas_cols = ['cicid','arrdate', 'i94mon','i94yr','visatype','i94visa', 'i94cit','i94port',\n",
    "                'i94addr','i94mode', 'airline','fltno','gender','biryear','depdate']    \n",
    "    new_sas_cols_dict = {'arrdate':'arrival_date', 'i94mon':'month','i94yr':'year','visatype':'visa_category','i94visa':'visa_type',\n",
    "                         'i94cit':'country_of_departure','i94port':'port_of_entry','i94addr':'us_address_state','i94mode':'mode_of_entry',\n",
    "                         'airline':'airline','fltno':'flight_no','biryear':'birth_year','depdate':'departure_date_from_us'}\n",
    "    \n",
    "   \n",
    "    sas_df = pd.DataFrame(columns=sas_cols)  #create a pandas df which will hold SAS data   \n",
    "    months_list = [(x[0:3]).lower() for x in calendar.month_name[1:]] #list all months in a year\n",
    "    \n",
    "    #Read immigration files from disk, transform to desired schema and save the files to S3 in paraquet format\n",
    "    for month in months_list[3:4]:   #remove slicing([3:4]) to retrieve data for all months, this slicing returns month april only\n",
    "        file_path = f\"{sas_input_path_prefix}{month}{sas_input_path_suffix}\"\n",
    "        print(f\"Reading file:{file_path}\")        \n",
    "        df = pd.read_sas(file_path, 'sas7bdat', encoding=\"ISO-8859-1\")\n",
    "        df =  df.loc[:,sas_cols]\n",
    "        sas_df = sas_df.append(df,ignore_index= True)\n",
    "        print(f'{file_path} file read sucessfully!')\n",
    "    \n",
    "    #Data Transformation       \n",
    "    sas_df.rename(columns=new_sas_cols_dict, inplace=True) #Rename the columns as per data model  \n",
    "    sas_df['arrival_date'] = pd.to_datetime(sas_df['arrival_date'], unit='D', origin='1960-01-01') #Convert SAS Date\n",
    "    sas_df['departure_date_from_us'] = pd.to_datetime(sas_df['departure_date_from_us'], unit='D', origin='1960-01-01') #Convert SAS Date\n",
    "    \n",
    "    \n",
    "    #code to extract immigration_df for fact and dimension tables\n",
    "    immigration_df = sas_df.to_csv(\"fact_immigration\")\n",
    "    immigration_df = spark.read.option(\"header\",\"true\").csv(\"fact_immigration\")\n",
    "    \n",
    "    # extract columns to create fact_immigration table and write parquet file to s3\n",
    "    fact_immigration= immigration_df.select(['cicid', 'arrival_date', 'visa_type','port_of_entry', 'us_address_state','country_of_departure','mode_of_entry','flight_no','gender','birth_year','departure_date_from_us'])\n",
    "    fact_immigration.write.parquet(os.path.join(output_path,'fact_immigration'),mode = \"overwrite\", partitionBy='arrival_date')\n",
    "\n",
    "    # extract columns to create dim_flight table and write parquet file to s3\n",
    "    flight_df = immigration_df.select(['flight_no','airline'])\n",
    "    flight_df = flight_df.drop_duplicates(subset=['flight_no']) #drop duplicates\n",
    "    flight_df = flight_df.filter(flight_df.flight_no.isNotNull()) #filter df to exclude null values\n",
    "    flight_df.write.parquet(os.path.join(output_path,'dim_flight'),mode = \"overwrite\")\n",
    "    display(flight_df.show(3))\n",
    "\n",
    "\n",
    "    # extract columns to create dim_calendar table and write parquet file to s3\n",
    "    time_df = immigration_df.select('arrival_date','year','month',  \n",
    "                 F.dayofmonth(\"arrival_date\").alias('day'),\n",
    "                 F.weekofyear(\"arrival_date\").alias('week'), \n",
    "                 F.date_format(F.col(\"arrival_date\"), \"E\").alias(\"weekday\"))\n",
    "    \n",
    "    time_df = time_df.drop_duplicates(subset=['arrival_date']) #drop duplicates\n",
    "    time_df.write.parquet(os.path.join(output_path,'dim_calendar'),mode = \"overwrite\", partitionBy=['year', 'month'])\n",
    "    display(time_df.show(3))\n",
    "    print('*****SAS data files processed!*******')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "###### Temperature Data : To withdraw latest temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def processTempData(filepath):\n",
    "    '''\n",
    "        This function reads temperature data, filters US cities data transforms it and returns temperature dataframe.\n",
    "        Input:  input path\n",
    "        Output: pandas df\n",
    "    '''\n",
    "    print(f\"Reading file:{filepath}\")  \n",
    "    temp_df = pd.read_csv(filepath) #read temperature data\n",
    "    print(f'{filepath} file read sucessfully!')\n",
    "    \n",
    "    temp_df = temp_df.dropna() #drop null values\n",
    "    col_dict = {temp_df.columns[1]:'avg_temperature', 'dt':'recorded_temp_date', temp_df.columns[2]:'avg_temp_uncertainity'}\n",
    "    temp_df = temp_df.rename(columns = col_dict) #rename columns as per data model\n",
    "    temp_df.columns = temp_df.columns.str.lower() #transform df as per data model\n",
    "    temp_df['recorded_temp_date'] = pd.to_datetime(temp_df['recorded_temp_date']) #convert to datetime object\n",
    "    \n",
    "      \n",
    "    temp_df = temp_df[temp_df.country ==\"United States\"] #filter df for US only\n",
    "    # fetch latest temp records only\n",
    "    temp_df = temp_df[temp_df.groupby('city').recorded_temp_date.transform('max')==temp_df['recorded_temp_date']] \n",
    "    temp_df.reset_index(inplace=True, drop=True)\n",
    "    print('*****Temperature file processed!*******')    \n",
    "    return temp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "###### US Cities Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def processUSCitiesData(temp_df,filepath,spark,output_path):\n",
    "    '''\n",
    "        This function reads us cities demographics data,processes it, merges it with temperature data and saves it parquet format to create fact_us_cities_demographics table.\n",
    "        Input: temperature df, input path, spark session object, S3 output path.\n",
    "        Output: Print out result.\n",
    "    '''    \n",
    "    #Read the file and transform the header names using a list of new headers\n",
    "    print(f\"Reading file:{filepath}\")  \n",
    "    names=['city','state_name','median_age','male_population','female_population','total_population','no_of_veterans','foriegn_born','avg_household_size','state_code','race','count']\n",
    "    city_df = pd.read_csv(filepath,sep = ';',names=names,header=0)\n",
    "    print(f'{filepath} file read sucessfully!')\n",
    "    \n",
    "    city_df.dropna(inplace=True) #drop rows with NaN values\n",
    "    city_df = city_df[names[0:-3]] #filter DF for required columns only as per data model \n",
    "    city_df = pd.merge(city_df, temp_df, how=\"left\", on='city') #merge city_df with input temperature DF\n",
    "    city_df = city_df[city_df.columns[0:-3]] #filter DF for required columns only as per data model\n",
    "    \n",
    "    #code to transform pandas df to spark df and save in parquet format on s3\n",
    "    city_df = spark.createDataFrame(city_df)\n",
    "    city_df = city_df.drop_duplicates(['city','state_name']) #drop duplicates\n",
    "    city_df.write.mode(\"overwrite\").parquet(os.path.join(output_path,'fact_us_cities_demographics'))\n",
    "    print('*****US Cities Demographics file processed with temperature data!*******')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "###### Airport Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def processAirportData(filepath,spark,output_path):\n",
    "    '''\n",
    "        This function reads us airport data, transforms data to handle null values,rename cols, drops duplicates and saves it parquet format to create fact_airport table.\n",
    "        Input: input path, spark session object, S3 output path.\n",
    "        Output: Print out result.\n",
    "    ''' \n",
    "    print(f\"Reading file:{filepath}\")  \n",
    "    airport_df = pd.read_csv(filepath) #read airport data\n",
    "    print(f'{filepath} file read sucessfully!')    \n",
    "\n",
    "    #Fill continent as nan : North America if iso_country in 'US' ir 'CA', like other US or Canada regions data already present in df \n",
    "    convert_dict ={'continent':str,'gps_code':str, 'local_code':str,'municipality':str,'iso_country':str}\n",
    "    airport_df=airport_df.astype(convert_dict)\n",
    "    airport_df.rename(columns = {'ident':'airport_id'},inplace=True) #rename col as per data model\n",
    "    airport_df['continent'] = np.where(airport_df.iso_country == ('US'or'CA') ,'nan',airport_df['continent'])\n",
    "\n",
    "    #Split coloumn coordinates into two new columns lat and long \n",
    "    airport_df[['latitude','longitude']] = airport_df['coordinates'].str.split(',', expand=True)\n",
    "    airport_df['latitude']= airport_df.latitude.apply(lambda x : round(float(x),2))\n",
    "    airport_df['longitude']= airport_df.longitude.apply(lambda x : round(float(x),2))\n",
    "\n",
    "    #Drop column iata_code as more than 80% records are null for this column and drop coordinates column\n",
    "    airport_df.drop(labels=['iata_code','coordinates'], axis = 1, inplace=True)\n",
    "    airport_df.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    #code to load csv file to S3\n",
    "    airport_df = spark.createDataFrame(airport_df)\n",
    "    airport_df = airport_df.drop_duplicates(subset=['airport_id'])\n",
    "    airport_df.write.mode(\"overwrite\").parquet(os.path.join(output_path,'fact_airport'))\n",
    "    print('*****Airport codes file processed!*******') \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "###### SAS Description Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def processSasDescriptionFile(filepath,spark,output_path):\n",
    "    '''\n",
    "        This function reads SAS Description file, extracts columns,rename cols, save it parquet format \n",
    "        to create dim_visa, dim_travelmode, dim_country, dim_state.\n",
    "        Input: input path, spark session object, S3 output path.\n",
    "        Output: Print out result.\n",
    "    '''     \n",
    "    sas_dict={}\n",
    "    sas_data = []\n",
    "    \n",
    "    print(f\"Processing file:{filepath}\")  \n",
    "    with open(filepath, \"r\") as file:\n",
    "        for line in file:\n",
    "            line = re.sub(r\"\\s+\", \" \", line)\n",
    "            if \"/*\" in line and \"-\" in line:\n",
    "                k, v = [i.strip(\" \") for i in line.split(\"*\")[1].split(\"-\", 1)]\n",
    "                k = k.replace(' & ', '_').lower()\n",
    "                sas_dict[k] = {'description': v}\n",
    "                \n",
    "            elif '=' in line and ';' not in line:\n",
    "                sas_data.append([i.strip(' ').strip(\"'\").title() for i in line.split('=')])\n",
    "            \n",
    "            elif len(sas_data) > 0:\n",
    "                sas_dict[k]['data'] = sas_data\n",
    "                sas_data = []\n",
    "\n",
    "    #To create dim_country           \n",
    "    country_df = spark.createDataFrame(sas_dict['i94cit_i94res']['data'], schema=['country_code', 'country_name'])\n",
    "    country_df.write.mode(\"overwrite\").parquet(os.path.join(output_path,'dim_country')) #load parquet file to S3\n",
    "    display(country_df.show(3))\n",
    "    print(\"dim_country created!\")\n",
    "\n",
    "    #To create dim_state\n",
    "    state_df = spark.createDataFrame(sas_dict['i94addr']['data'],schema=['state_code', 'state_name'])\n",
    "    state_df = state_df.withColumn('state_code', upper(state_df.state_code))\n",
    "    state_df.write.mode(\"overwrite\").parquet(os.path.join(output_path,'dim_state')) #load parquet file to S3\n",
    "    display(state_df.show(3))\n",
    "    print(\"dim_state created!\")\n",
    "    \n",
    "    #To create dim_visa  \n",
    "    visa_df = spark.createDataFrame(sas_dict['i94visa']['data'], schema=['visa_type_id', 'visa_category'])\n",
    "    visa_df.write.mode(\"overwrite\").parquet(os.path.join(output_path,'dim_visa')) #load parquet file to S3\n",
    "    display(visa_df.show(3))\n",
    "    print(\"dim_visa created!\")\n",
    "\n",
    "    #To create dim_travelmode\n",
    "    travel_mode_df = spark.createDataFrame(sas_dict['i94mode']['data'], schema=['mode_type_id', 'mode_category'])\n",
    "    travel_mode_df.write.mode(\"overwrite\").parquet(os.path.join(output_path,'dim_travelmode')) #load parquet file to S3\n",
    "    display(travel_mode_df.show(3))\n",
    "    print(\"dim_travelmode created!\")\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks \n",
    "Data quality for the created records has been checked by:\n",
    "* **check_row_count()**: Checking the count of records in each of the tables, it should not be 0.\n",
    "* **check_for_primary_key()**: Checking for primary key constraints: Checking both Unique Key contraint, that column should have no duplicates and for the NULL constraint that column should not have any NULL values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def check_row_count(spark, output_path, table_list):\n",
    "    '''\n",
    "        This function counts the number of rows in all the tables in the list, if less than 0 raise error.\n",
    "        Input: Spark Session object, S3 path, list of tables.\n",
    "        Output: Print out result.\n",
    "    '''\n",
    "    print(\"Checking Data quality:for rowcount\")\n",
    "    \n",
    "    for table in table_list:\n",
    "        table_df = spark.read.parquet(os.path.join(output_path, table))\n",
    "        cnt = table_df.count()\n",
    "        if cnt == 0:\n",
    "            raise ValueError(f'Quality check FAILED for {table} with zero records.')\n",
    "        else:\n",
    "            print(f'Rowcount quality check PASSED for {table} with {cnt} records.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def check_for_primary_key(spark, output_path, table_dict):\n",
    "    '''\n",
    "        This function checks for primary key constraints: column should be unique and have no null records.\n",
    "        Input: Spark Session object, S3 path, dict of tables and columns.\n",
    "        Output: Print out check result.\n",
    "    '''\n",
    "    print(\"Checking Data quality:for primary key constraints\")\n",
    "    \n",
    "    for table,column in table_dict.items():\n",
    "        table_df = spark.read.parquet(os.path.join(output_path, table))        \n",
    "        if table_df.count() > table_df.dropDuplicates(''.join(column).strip(',').split(',')).count():\n",
    "            raise ValueError(f'Primary Key unique constraint check FAILED for table:{table} column:{column}')            \n",
    "        elif (not isinstance(column,list)) and (table_df.filter((table_df[column] == \"\") | table_df[column].isNull() | isnan(table_df[column])).count() > 0):\n",
    "            raise ValueError(f'Primary Key null constraint check FAILED for table:{table} column:{column}')            \n",
    "        else:\n",
    "            print(f'Primary Key constraints check PASSED for table:{table} column:{column}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETL process started!\n",
      "Reading file:../../data2/GlobalLandTemperaturesByCity.csv\n",
      "../../data2/GlobalLandTemperaturesByCity.csv file read sucessfully!\n",
      "*****Temperature file processed!*******\n",
      "Reading file:us-cities-demographics.csv\n",
      "us-cities-demographics.csv file read sucessfully!\n",
      "*****US Cities Demographics file processed with temperature data!*******\n",
      "Reading file:../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat\n",
      "../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat file read sucessfully!\n",
      "+---------+-------+\n",
      "|flight_no|airline|\n",
      "+---------+-------+\n",
      "|    00332|    YNT|\n",
      "|    00456|     LH|\n",
      "|    00530|     LA|\n",
      "+---------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+-----+---+----+-------+\n",
      "|arrival_date|  year|month|day|week|weekday|\n",
      "+------------+------+-----+---+----+-------+\n",
      "|  2016-04-22|2016.0|  4.0| 22|  16|    Fri|\n",
      "|  2016-04-15|2016.0|  4.0| 15|  15|    Fri|\n",
      "|  2016-04-18|2016.0|  4.0| 18|  16|    Mon|\n",
      "+------------+------+-----+---+----+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****SAS data files processed!*******\n",
      "Reading file:airport-codes_csv.csv\n",
      "airport-codes_csv.csv file read sucessfully!\n",
      "*****Airport codes file processed!*******\n",
      "Processing file:I94_SAS_Labels_Descriptions.SAS\n",
      "+------------+--------------------+\n",
      "|country_code|        country_name|\n",
      "+------------+--------------------+\n",
      "|         582|Mexico Air Sea, A...|\n",
      "|         236|         Afghanistan|\n",
      "|         101|             Albania|\n",
      "+------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim_country created!\n",
      "+----------+----------+\n",
      "|state_code|state_name|\n",
      "+----------+----------+\n",
      "|        AL|   Alabama|\n",
      "|        AK|    Alaska|\n",
      "|        AZ|   Arizona|\n",
      "+----------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim_state created!\n",
      "+------------+-------------+\n",
      "|visa_type_id|visa_category|\n",
      "+------------+-------------+\n",
      "|           1|     Business|\n",
      "|           2|     Pleasure|\n",
      "|           3|      Student|\n",
      "+------------+-------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim_visa created!\n",
      "+------------+-------------+\n",
      "|mode_type_id|mode_category|\n",
      "+------------+-------------+\n",
      "|           1|          Air|\n",
      "|           2|          Sea|\n",
      "|           3|         Land|\n",
      "+------------+-------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim_travelmode created!\n",
      "Checking Data quality:for rowcount\n",
      "Rowcount quality check PASSED for fact_us_cities_demographics with 588 records.\n",
      "Rowcount quality check PASSED for fact_immigration with 3096313 records.\n",
      "Rowcount quality check PASSED for fact_airport with 55075 records.\n",
      "Rowcount quality check PASSED for dim_calendar with 30 records.\n",
      "Rowcount quality check PASSED for dim_flight with 7152 records.\n",
      "Rowcount quality check PASSED for dim_visa with 3 records.\n",
      "Rowcount quality check PASSED for dim_country with 288 records.\n",
      "Rowcount quality check PASSED for dim_state with 54 records.\n",
      "Rowcount quality check PASSED for dim_travelmode with 3 records.\n",
      "Checking Data quality:for primary key constraints\n",
      "Primary Key constraints check PASSED for table:fact_us_cities_demographics column:['city,state_name']\n",
      "Primary Key constraints check PASSED for table:fact_immigration column:cicid\n",
      "Primary Key constraints check PASSED for table:fact_airport column:airport_id\n",
      "Primary Key constraints check PASSED for table:dim_calendar column:arrival_date\n",
      "Primary Key constraints check PASSED for table:dim_flight column:flight_no\n",
      "Primary Key constraints check PASSED for table:dim_visa column:visa_type_id\n",
      "Primary Key constraints check PASSED for table:dim_country column:country_code\n",
      "Primary Key constraints check PASSED for table:dim_state column:state_code\n",
      "Primary Key constraints check PASSED for table:dim_travelmode column:mode_type_id\n",
      "ETL process completed!\n"
     ]
    }
   ],
   "source": [
    "print(\"ETL process started!\")\n",
    "    \n",
    "spark = createSparkSession() #create spark session\n",
    "output_path = 's3a://juhi-capstone/'\n",
    "sas_input_path_prefix = '../../data/18-83510-I94-Data-2016/i94_'\n",
    "sas_input_path_suffix = '16_sub.sas7bdat'\n",
    "table_dict = {'fact_us_cities_demographics': ['city,state_name'],'fact_immigration':'cicid','fact_airport':'airport_id',\n",
    "             'dim_calendar':'arrival_date','dim_flight':'flight_no','dim_visa':'visa_type_id','dim_country':'country_code',\n",
    "              'dim_state':'state_code','dim_travelmode':'mode_type_id'}\n",
    "\n",
    "#Process temperature data and US cities demographics data to create fact table: fact_us_cities_demographics\n",
    "temp_df = processTempData('../../data2/GlobalLandTemperaturesByCity.csv')\n",
    "processUSCitiesData(temp_df,'us-cities-demographics.csv',spark,output_path)\n",
    "    \n",
    "#Process sas data to create fact table: fact_immigration and dimension tables : dim_calendar and dim_flight\n",
    "processUSImmigrationData(sas_input_path_prefix,sas_input_path_suffix,spark,output_path)\n",
    "    \n",
    "#Process airport data to create fact table: fact_airport\n",
    "processAirportData('airport-codes_csv.csv',spark,output_path)\n",
    "    \n",
    "#Process SAS Description file to create dimensions: dim_country, dim_state, dim_visa, dim_travelmode\n",
    "processSasDescriptionFile(\"I94_SAS_Labels_Descriptions.SAS\",spark,output_path)\n",
    "    \n",
    "#Data quality check: check for rowcount\n",
    "check_row_count(spark, output_path,table_dict.keys())\n",
    "\n",
    "#Data quality check: check for Primary key constraints: Unique Key and Null value check\n",
    "check_for_primary_key(spark, output_path, table_dict)\n",
    "\n",
    "print(\"ETL process completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### <u>fact_immigration</u>: Fact table extracted from the I94 immigration SAS data.\n",
    "* `cicid` = key id\n",
    "* `arrival_date` = arrival date\n",
    "* `visa_type` = 1 digit visa type code\n",
    "* `port_of_entry` = 3 character code of destination city\n",
    "* `country_of_departure` = 3 digit code of origin country\n",
    "* `mode_of_entry` = 1 digit transportation mode code\n",
    "* `flight_number` = flight code taken by the traveller\n",
    "* `gender` = gender of traveller \n",
    "* `birth_year` = Age of traveller\n",
    "* `departure_date_from_us` = depature date from US\n",
    "\n",
    "<br>\n",
    "\n",
    "##### <u>fact_us_cities_demographics</u>: Fact table extracted from US cities demographics dataset joined with temperature dataset.\n",
    "* `city_name` = US city name\n",
    "* `state_code` = US state code\n",
    "* `median_age` = median age\n",
    "* `male_population` = male population\n",
    "* `female_population` = female population\n",
    "* `total_population` = total population\n",
    "* `no_of_veterans` = number of veterans\n",
    "* `foreign_born` = number of foreign born\n",
    "* `avg_household_size` = average household size\n",
    "* `recorded_temp_date` = latest date on which temperature was recorded\n",
    "* `avg_temperature` = average temperature on the recorded_temp_date\n",
    "* `avg_temp_uncertainity` = average temperature uncertainity on the recorded_temp_date\n",
    "\n",
    "<br>\n",
    "\n",
    "##### <u>fact_airport</u>: Fact table extracted from airport codes dataset.\n",
    "* `airport_id` = airport id code\n",
    "* `airport_type` = Type of the airport (small, medium, large)\n",
    "* `airport_name` = name of the airport\n",
    "* `elevation_ft` = elevation height of the airport\n",
    "* `continent` = continent in which airport is located\n",
    "* `iso_country` = 2 letter code for country in which airport is located\n",
    "* `iso_region` =  location in which airport is located\n",
    "* `municipality` = location in which airport is located\n",
    "* `gps_code` =  4 letter gps_code in which airport is located\n",
    "* `local_code` = 4 letter local code in which airport is located\n",
    "* `latitude` =  latitude for airport location\n",
    "* `longitude` = longitude for airport location\n",
    "\n",
    "<br>\n",
    "\n",
    "##### <u>dim_calendar</u>: Dimension table extracted from the I94 immigration SAS data.\n",
    "* `arrival_date` = arrival date\n",
    "* `year` = arrival year\n",
    "* `month` = arrival month\n",
    "* `day` = arrival day of month\n",
    "* `week` = arrival week of year\n",
    "* `weekday` = arrival weekday\n",
    "\n",
    "<br>\n",
    "\n",
    "##### <u>dim_flight</u>: Dimension table extracted from the I94 immigration SAS data.\n",
    "* `flight_no` = flight number\n",
    "* `airline_name` = airline name\n",
    "\n",
    "<br>\n",
    "\n",
    "##### <u>dim_visa</u>: Dimension table extracted from I94_SAS_Labels_Descriptions.SAS\n",
    "* `visa_type_id` = 1 digit code of visa type\n",
    "* `visa_category` = Type of visa (business, pleasure, student)\n",
    "\n",
    "<br>\n",
    "\n",
    "##### <u>dim_travelmode</u>: Dimension table extracted from I94_SAS_Labels_Descriptions.SAS\n",
    "* `mode_code` = 1 digit code of transportation mode\n",
    "* `mode` = Mode of transportation (air, land, sea)\n",
    "\n",
    "<br>\n",
    "\n",
    "##### <u>dim_countries</u>: Dimension table extracted from I94_SAS_Labels_Descriptions.SAS\n",
    "* `country_code` = 3 digit country code \n",
    "* `country_name` = country name\n",
    "\n",
    "<br>\n",
    "\n",
    "##### <u>dim_state</u>: Dimension table extracted from I94_SAS_Labels_Descriptions.SAS\n",
    "* `state_code` = 2 digit state code \n",
    "* `state_name` = state name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "\n",
    "* #### Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "As discussed above, we have used Python libraries, PySpark and AWS S3 to build ETL pipline and datalake as per the defined data model. Python file handling functions and Pandas works efficiently for small files and makes it convinient to work with multiple dataframes. And to process and save large volume dataset like i94 immigration data which contains over 3 million rows per month, Apache Spark has been used, it has many easy-to-use APIs and it executes much faster by caching data in memory across multiple parallel operations. Finally, datalake has been implemented on AWS S3 to store the fact and dimension tables with partioning for efficient and fast storage and retrival process.\n",
    "\n",
    "* #### Propose how often the data should be updated and why.\n",
    "The data should be updated whenever raw data is created from the data sources.\n",
    "In this project, dimension tables are created from SAS Immigration data source, so dimension tables are to be updated only when new visa category, mode of entries, cities or countries are added to Immigration data. But for fact_immigration SAS data is added monthly so data updation should be done monthly, similarly dim_flight and dim_calendar will be uppdated accordingly for new date and flight details correspondingly.\n",
    "The US Cities Demographics data is updated in every ten years(reference: https://www.usa.gov/statistics). So, the fact_us_cities_demographics and fact_airport are required to be updated whenever new records are generated by their data sources.\n",
    "    \n",
    "\n",
    "* #### Write a description of how you would approach the problem differently under the following scenarios:\n",
    "\n",
    " * ##### The data was increased by 100x.\n",
    "     We are currently executing code in local mode, but by increasing the number of nodes in the cluster,\n",
    "     spark will be able handle the increasing amount of data. However, an efficient approch would be to deploy this\n",
    "     solution on AWS EMR Cluster. AWS supports easy scaling for this much data.<br>\n",
    " * ##### The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "     If data was to be updated daily scheduling jobs would be an efficient solution.\n",
    "     We can use any of the scheduling options available like cronjobs or serverless solutions AWS Glue, AWS lambda to \n",
    "     process code every time data is recieved. Another, very efficient solution for this use case would be using \n",
    "     scheduling tool Airflow.<br>\n",
    " * ##### The database needed to be accessed by 100+ people.\n",
    "     We could move the analytics database solution to AWS Redshift Cluster, it can easily scale and can handle increasing\n",
    "     data access requirements by multiple users. It is efficient to handle high concurrent and parallel access by large \n",
    "     amount of users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Sample Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|City| count|\n",
      "+----+------+\n",
      "| NYC|485916|\n",
      "| MIA|343941|\n",
      "| LOS|310163|\n",
      "| SFR|152586|\n",
      "| ORL|149195|\n",
      "| HHW|142720|\n",
      "| NEW|136122|\n",
      "| CHI|130564|\n",
      "| HOU|101481|\n",
      "| FTL| 95977|\n",
      "+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Top 10 cities used as port of entry\n",
    "immigration_df = spark.read.parquet(\"s3a://juhi-capstone/fact_immigration\")\n",
    "immigration_df.createOrReplaceTempView(\"fact_immigration\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        port_of_entry as City,\n",
    "        count(*) as count\n",
    "    FROM\n",
    "        fact_immigration\n",
    "    GROUP BY port_of_entry\n",
    "    ORDER BY 2 desc\n",
    "    LIMIT 10\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+-------+\n",
      "|visa_type_id|visa_category|  count|\n",
      "+------------+-------------+-------+\n",
      "|           2|     Pleasure|2530868|\n",
      "|           1|     Business| 522079|\n",
      "|           3|      Student|  43366|\n",
      "+------------+-------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Most common reason for immigrants to travel to US\n",
    "visa_df = spark.read.parquet(\"s3a://juhi-capstone/dim_visa\")\n",
    "visa_df.createOrReplaceTempView(\"dim_visa\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        v.visa_type_id,\n",
    "        v.visa_category,\n",
    "        count(i.visa_type) as count\n",
    "    FROM\n",
    "        fact_immigration i,\n",
    "        dim_visa v\n",
    "    WHERE int(i.visa_type) = v.visa_type_id\n",
    "    GROUP BY 1,2\n",
    "    ORDER BY 3 desc\n",
    "\"\"\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
